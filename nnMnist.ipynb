{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# import keras\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABHdJREFUeJzt3TFOlF0UgGHnz4SEjsYEG21lCRTEWNGzC5fgJihZB8E90BFKaChoiIUmUBG0GLu/gov6wYzhfZ5yTsa5Cb45CTcfM1ssFq+Al++/VR8AWA6xQ4TYIULsECF2iJgv+fP86h+e3+y+F212iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUPEfNUH4Hn9/PlzOD8+Ph7OP3/+POn9/DtsdogQO0SIHSLEDhFihwixQ4TYIWK2WCyW+XlL/TBevfr27dtw/vr16+F8c3NzOD89PZ30fp7F7L4XbXaIEDtEiB0ixA4RYocIsUOER1wZ+vr166S5q7d/h80OEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0R4np1Jbm9vV30EfpPNDhFihwixQ4TYIULsECF2iBA7RLhnZ5KTk5PhfHt7e0kn4TE2O0SIHSLEDhFihwixQ4TYIULsEOGe/YWbz8c/4o2NjeH8+vp6OL+4uPjjM7EaNjtEiB0ixA4RYocIsUOE2CHC1dsL99jV2s7OznD+5cuXpzwOK2SzQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4Rnmdnku/fv6/6CPwmmx0ixA4RYocIsUOE2CFC7BAhdohwz84kR0dHqz4Cv8lmhwixQ4TYIULsECF2iBA7RIgdItyzx338+HE49/3sL4fNDhFihwixQ4TYIULsECF2iHD1Fvf27dtJ7//x48dwfnl5+eDs3bt3kz6bP2OzQ4TYIULsECF2iBA7RIgdIsQOEe7Z4+bzaf8FFovFcH53dzfp3+fp2OwQIXaIEDtEiB0ixA4RYocIsUPE7LF70ie21A9juq2treH8/Px8OP/06dODs4ODg786E4+a3feizQ4RYocIsUOE2CFC7BAhdogQO0R4np2h3d3d4fzq6mo439/ff8rjMIHNDhFihwixQ4TYIULsECF2iHD1xiSz2b1PU/5vbW1tSSfhMTY7RIgdIsQOEWKHCLFDhNghQuwQ4Z6dSW5ubobzw8PDB2d7e3tPfRwGbHaIEDtEiB0ixA4RYocIsUOE2CHCVzYz9ObNm+H8+vp6OD89PX1w9v79+786E4/ylc1QJnaIEDtEiB0ixA4RYocIsUOE59kZ+vDhw3B+dnY2nK+vrz/lcZjAZocIsUOE2CFC7BAhdogQO0SIHSI8zw4vj+fZoUzsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhHL/srme//ELfD8bHaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNgh4hdEKmfD6e9v1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 8\n",
    "plt.imshow(x_train[i,:], cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "\n",
    "image_vector_size = 28*28\n",
    "\n",
    "X_train = x_train.reshape(x_train.shape[0],image_vector_size)\n",
    "\n",
    "X_test = x_test.reshape( x_test.shape[0],image_vector_size)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "# scale\n",
    "X_train = X_train / 255\n",
    "X_test = X_test/ 255\n",
    "\n",
    "\n",
    "# one-hot encode labels\n",
    "# Convert to \"one-hot\" vectors using the to_categorical function\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_train=y_train.T\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_test=y_test.T\n",
    "Y_test=y_test\n",
    "\n",
    "# split, reshape, shuffle\n",
    "m = 60000\n",
    "#m_test = X.shape[0] - m\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 8\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y_train[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def compute_loss(Y, Y_hat):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L\n",
    "\n",
    "def feed_forward(X, params,drop_p):\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    #keep_prob=1-drop_p\n",
    "    #D1 = np.random.rand(cache[\"A1\"].shape[0], cache[\"A1\"].shape[1])                               # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    #D1 = D1 < keep_prob                                         # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    #cache[\"A1\"] = np.multiply(cache[\"A1\"], D1)                                         # Step 3: shut down some neurons of A1\n",
    "    #cache[\"A1\"] = cache[\"A1\"]/keep_prob   \n",
    "    #cache[\"D1\"] = D1\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache\n",
    "\n",
    "def back_propagate(X, Y, params, cache,drop_p):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    #keep_prob=1-drop_p\n",
    "    #dA1= np.multiply(cache[\"D1\"],dA1)\n",
    "    #dA1 = dA1/keep_prob\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training cost = 0.1557111511071778, test cost = 0.16695714193366465\n",
      "Epoch 2: training cost = 0.10092936073593455, test cost = 0.11760574469284768\n",
      "Epoch 3: training cost = 0.0806483930222924, test cost = 0.10210363241511437\n",
      "Epoch 4: training cost = 0.06818503106233023, test cost = 0.10215030083919488\n",
      "Epoch 5: training cost = 0.059403118954138544, test cost = 0.0997087884486664\n",
      "Epoch 6: training cost = 0.043732593499217244, test cost = 0.0931807548622698\n",
      "Epoch 7: training cost = 0.03915062730444597, test cost = 0.09326284544354708\n",
      "Epoch 8: training cost = 0.034101018883718234, test cost = 0.08950138292981233\n",
      "Epoch 9: training cost = 0.03419568745575665, test cost = 0.09364669318744087\n",
      "Epoch 10: training cost = 0.02630328043497226, test cost = 0.0891818202006092\n",
      "Epoch 11: training cost = 0.022091965141241947, test cost = 0.08997185223175606\n",
      "Epoch 12: training cost = 0.019193324323089917, test cost = 0.08776972817211459\n",
      "Epoch 13: training cost = 0.017442822687971853, test cost = 0.0910365115364836\n",
      "Epoch 14: training cost = 0.013057536958777566, test cost = 0.08861472491612343\n",
      "Epoch 15: training cost = 0.013975121115147938, test cost = 0.08970996024055772\n",
      "Epoch 16: training cost = 0.012939335800090893, test cost = 0.09384204759724657\n",
      "Epoch 17: training cost = 0.010454100173474843, test cost = 0.09121119585046336\n",
      "Epoch 18: training cost = 0.007392910325015019, test cost = 0.08946408719855517\n",
      "Epoch 19: training cost = 0.007325779178159502, test cost = 0.09072634138502349\n",
      "Epoch 20: training cost = 0.007308108380628153, test cost = 0.0984699128048145\n",
      "Epoch 21: training cost = 0.005985847958953466, test cost = 0.09285779050936423\n",
      "Epoch 22: training cost = 0.004866019010333571, test cost = 0.09431813014558262\n",
      "Epoch 23: training cost = 0.004517409721716697, test cost = 0.09384710841314697\n",
      "Epoch 24: training cost = 0.00381816118373243, test cost = 0.09416945049513001\n",
      "Epoch 25: training cost = 0.0034695316891543973, test cost = 0.0926372083203103\n",
      "Epoch 26: training cost = 0.0033126615257767457, test cost = 0.09254022641813907\n",
      "Epoch 27: training cost = 0.0031878292244426113, test cost = 0.09595648067125764\n",
      "Epoch 28: training cost = 0.0032991832242595383, test cost = 0.09656051902145504\n",
      "Epoch 29: training cost = 0.0026419884214034764, test cost = 0.09703066549946535\n",
      "Epoch 30: training cost = 0.0025165901146227206, test cost = 0.09482443320890561\n",
      "Epoch 31: training cost = 0.002218601629718637, test cost = 0.09726208684063324\n",
      "Epoch 32: training cost = 0.002206772203148297, test cost = 0.09635665206544967\n",
      "Epoch 33: training cost = 0.0020107899858506947, test cost = 0.0960773355510907\n",
      "Epoch 34: training cost = 0.0020371492565411896, test cost = 0.09737790095265576\n",
      "Epoch 35: training cost = 0.0018463915584191418, test cost = 0.09700583950144175\n",
      "Epoch 36: training cost = 0.0017660493179387615, test cost = 0.09849218531713158\n",
      "Epoch 37: training cost = 0.0017463613827659855, test cost = 0.0982917858466835\n",
      "Epoch 38: training cost = 0.0017480750328499545, test cost = 0.09892495679331846\n",
      "Epoch 39: training cost = 0.0015783255929208285, test cost = 0.09898874756179242\n",
      "Epoch 40: training cost = 0.0015228494425866476, test cost = 0.0999187370861315\n",
      "Epoch 41: training cost = 0.0014678260770274393, test cost = 0.10071374154269087\n",
      "Epoch 42: training cost = 0.0014161498231441932, test cost = 0.10118400041309984\n",
      "Epoch 43: training cost = 0.0013638553681765621, test cost = 0.09964937811919443\n",
      "Epoch 44: training cost = 0.0013946216736292127, test cost = 0.10089019832535423\n",
      "Epoch 45: training cost = 0.001292033013262848, test cost = 0.10139782038479347\n",
      "Epoch 46: training cost = 0.0012546175652693313, test cost = 0.10153994030432427\n",
      "Epoch 47: training cost = 0.0012044683505318342, test cost = 0.10218470492210202\n",
      "Epoch 48: training cost = 0.0011820712758694632, test cost = 0.10194086288843093\n",
      "Epoch 49: training cost = 0.00115335215745925, test cost = 0.102625524881392\n",
      "Epoch 50: training cost = 0.001116683958926778, test cost = 0.1028576900754039\n",
      "Epoch 51: training cost = 0.001149699339366858, test cost = 0.10253530678192015\n",
      "Epoch 52: training cost = 0.0010827474531442888, test cost = 0.1034236891570545\n",
      "Epoch 53: training cost = 0.0010340186821185505, test cost = 0.10357025074903625\n",
      "Epoch 54: training cost = 0.0010226896070688577, test cost = 0.10487585174490992\n",
      "Epoch 55: training cost = 0.000996223662891216, test cost = 0.10400393663290618\n",
      "Epoch 56: training cost = 0.0009588322795325842, test cost = 0.10467842014215549\n",
      "Epoch 57: training cost = 0.0009490886555780139, test cost = 0.10497429850687912\n",
      "Epoch 58: training cost = 0.0009324190587742388, test cost = 0.10460341041290513\n",
      "Epoch 59: training cost = 0.0009070843946534583, test cost = 0.10547424356033878\n",
      "Epoch 60: training cost = 0.0008872055345027139, test cost = 0.10572242991512835\n",
      "Epoch 61: training cost = 0.0008629628017156535, test cost = 0.10622017736695465\n",
      "Epoch 62: training cost = 0.0008491525682488466, test cost = 0.1055002713700454\n",
      "Epoch 63: training cost = 0.0008383872066062085, test cost = 0.10566606114683144\n",
      "Epoch 64: training cost = 0.0008145129652718464, test cost = 0.10639488238951345\n",
      "Epoch 65: training cost = 0.0008163520764883711, test cost = 0.10701332166200007\n",
      "Epoch 66: training cost = 0.0007808401817668701, test cost = 0.10676474096690917\n",
      "Epoch 67: training cost = 0.0007666614621065872, test cost = 0.10709573394012044\n",
      "Epoch 68: training cost = 0.0007528015457812421, test cost = 0.10703909107165191\n",
      "Epoch 69: training cost = 0.0007417718539958432, test cost = 0.10804272590499019\n",
      "Epoch 70: training cost = 0.0007424854087021088, test cost = 0.1081166034962778\n",
      "Epoch 71: training cost = 0.000715722851954289, test cost = 0.10728640288885673\n",
      "Epoch 72: training cost = 0.0007038808985227272, test cost = 0.10799944029600145\n",
      "Epoch 73: training cost = 0.0006898635405172078, test cost = 0.10847911655107126\n",
      "Epoch 74: training cost = 0.0006779311355748953, test cost = 0.10832624489537582\n",
      "Epoch 75: training cost = 0.0006814395292656535, test cost = 0.1082592893425077\n",
      "Epoch 76: training cost = 0.0006601099926359736, test cost = 0.10841421678001305\n",
      "Epoch 77: training cost = 0.0006523410698202832, test cost = 0.10936567949851299\n",
      "Epoch 78: training cost = 0.0006471947642970486, test cost = 0.10940073444314449\n",
      "Epoch 79: training cost = 0.0006243917446564454, test cost = 0.10972289721858218\n",
      "Epoch 80: training cost = 0.0006214760215172216, test cost = 0.11037966639087611\n",
      "Epoch 81: training cost = 0.0006082656041995213, test cost = 0.109835904452911\n",
      "Epoch 82: training cost = 0.0006006575336180654, test cost = 0.10935511174036625\n",
      "Epoch 83: training cost = 0.0005923098376707733, test cost = 0.10959099695321979\n",
      "Epoch 84: training cost = 0.000586677122968661, test cost = 0.11047913695739145\n",
      "Epoch 85: training cost = 0.0005767873983697655, test cost = 0.1108024350587753\n",
      "Epoch 86: training cost = 0.0005649279064040199, test cost = 0.11048633533013019\n",
      "Epoch 87: training cost = 0.0005598199040539824, test cost = 0.11097789399427455\n",
      "Epoch 88: training cost = 0.0005491106002515771, test cost = 0.11115410192291911\n",
      "Epoch 89: training cost = 0.0005454059423491823, test cost = 0.11128443783377402\n",
      "Epoch 90: training cost = 0.0005372140820987176, test cost = 0.11140751128428292\n",
      "Epoch 91: training cost = 0.000528920997995744, test cost = 0.11178122560182402\n",
      "Epoch 92: training cost = 0.0005199300230869699, test cost = 0.11144537168200623\n",
      "Epoch 93: training cost = 0.0005155959931065028, test cost = 0.11223052219602339\n",
      "Epoch 94: training cost = 0.0005096729585243012, test cost = 0.11187827766368849\n",
      "Epoch 95: training cost = 0.0005011289454996649, test cost = 0.11235274397969143\n",
      "Epoch 96: training cost = 0.00050640131890273, test cost = 0.11280787952385739\n",
      "Epoch 97: training cost = 0.0004899355468002476, test cost = 0.1123820858687483\n",
      "Epoch 98: training cost = 0.0004891020363130382, test cost = 0.11314542329899852\n",
      "Epoch 99: training cost = 0.00048570933447204367, test cost = 0.11214377636402897\n",
      "Epoch 100: training cost = 0.0004713109834058558, test cost = 0.11294484266719276\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(138)\n",
    "digits=10;\n",
    "# hyperparameters\n",
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "learning_rate = 4\n",
    "beta = 0.9\n",
    "batch_size = 128\n",
    "batches = -(-m // batch_size)\n",
    "drop_p=0.5\n",
    "\n",
    "# initialization\n",
    "params = { \"W1\": np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((n_h, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(digits, n_h) * np.sqrt(1. / n_h),\n",
    "           \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / n_h) }\n",
    "\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "\n",
    "prev_tcost=1000;\n",
    "# train\n",
    "for i in range(100):\n",
    "\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        cache = feed_forward(X, params,drop_p)\n",
    "        grads = back_propagate(X, Y, params, cache,drop_p)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "        params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "    cache = feed_forward(X_train, params,drop_p)\n",
    "    train_cost = compute_loss(Y_train, cache[\"A2\"])\n",
    "    cache = feed_forward(X_test, params,0.0)\n",
    "    test_cost = compute_loss(Y_test, cache[\"A2\"])\n",
    "    #if((prev_tcost+0.01)<test_cost):\n",
    "    #    break;\n",
    "    #else:\n",
    "    #    prev_tcost=test_cost\n",
    "    print(\"Epoch {}: training cost = {}, test cost = {}\".format(i+1 ,train_cost, test_cost))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       987\n",
      "           1       0.99      0.99      0.99      1137\n",
      "           2       0.97      0.98      0.97      1020\n",
      "           3       0.98      0.96      0.97      1028\n",
      "           4       0.98      0.98      0.98       981\n",
      "           5       0.97      0.97      0.97       887\n",
      "           6       0.98      0.98      0.98       957\n",
      "           7       0.97      0.97      0.97      1035\n",
      "           8       0.97      0.98      0.97       967\n",
      "           9       0.97      0.97      0.97      1001\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache = feed_forward(X_test, params,0.0)\n",
    "predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
